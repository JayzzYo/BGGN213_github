---
title: "Class08 Mini-project"
author: "Zixuan Zeng(A16142927)"
format: pdf
toc: true
---

## Background

The goal of today's mini-project is to perform principal component analysis (PCA) and hierarchical clustering on a dataset containing features of breast cancer tumors. 


## Data import


```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
wisc.data <- wisc.df[, -1]   
```

```{r}
diagnosis <- factor(wisc.df$diagnosis)
```

## Exploratory data analysis

```{r}
dim(wisc.df)
sum(diagnosis == "M")
sum(diagnosis == "B")
length(grep("_mean$",colnames(wisc.data)))
```

>Q1. 569 observations

>Q2. 212 observations with a malignant diagnosis

>Q3. 10 features


## Principal Component Analysis

The main function in base R for PCA is called `prcomp()`. An optional argument `scale` should nearly always be set to `TRUE` for this function.

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, center = TRUE, scale. = TRUE)
summary(wisc.pr)
```

>Q4. 44.27% of the variance is captured by PC1

>Q5. 3 PC required

>Q6. 7 PC required


## Interpreting PCA results

```{r}
biplot(wisc.pr)
```
>Q7. The variables are all over the place, making it extremly hard to understand visually.

```{r}
plot(wisc.pr$x[, 1:2], col = diagnosis ,
     xlab = "PC1", ylab = "PC2")
library(ggplot2)
ggplot(wisc.pr$x[, 1:2], aes(x=PC1, y=PC2, color=diagnosis)) + geom_point()
```

```{r}
plot(wisc.pr$x[, c(1,3)], col = diagnosis ,
     xlab = "PC1", ylab = "PC3")
```

>Q8. Black dots are clustered together while red dots are more spread out, indicating that benign tumors have more similarity within the group.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + aes(PC1, PC2, col=diagnosis) + geom_point()
```

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var/sum(pr.var)
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


```{r}
#Optional CRAN package:

library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

>Q9. -0.2608538. High concave point usually points to low PC1 value. 

```{r}
wisc.pr$rotation["concave.points_mean", 1]

```


## Hierarchical Clustering

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
abline(h = 20, col = "red", lty = 2)
```

>Q10. h=20



```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = (4))
table(wisc.hclust.clusters, diagnosis)

```

```{r}
for(i in 2:10){
  wisc.hclust.clusters.i <- cutree(wisc.hclust, k = (i))
  print(table(wisc.hclust.clusters.i, diagnosis))
}
```


>Q11. Either 4 or 5 clusters seem to provide the best separation.



>Q12. No method stands out as my favorite as the plot all appears messy with large data set. Results wise, ward.D2 seems to perform slightly better than other methods. 
TP=179 FP=24

```{r}
d <- dist(wisc.pr$x[,1:3]) #Finding distance between the first 3 columns of the PCA result.
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
abline(h=70, col="red")
grps <- cutree(wisc.pr.hclust, k=2)
table(grps, diagnosis)
```




```{r}
wisc.hclust.average <- hclust(data.dist,method="average")
plot(wisc.hclust.average)
```

```{r}
wisc.hclust.single <- hclust(data.dist,method="single")
plot(wisc.hclust.single)
```

```{r}
wisc.hclust.complete <- hclust(data.dist,method="complete")
plot(wisc.hclust.complete)
```



```{r}
wisc.pr.hclust <- hclust(data.dist,method="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

>Q13. This method doesn't work well with four clusters.

```{r}
d <- dist(wisc.pr$x[, 1:7], method = "euclidean")
wisc.pr.hclust <- hclust(d, method="ward.D2")
wisc.pr.hclust.clusters.2 <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters.2, diagnosis)
```

```{r}
wisc.pr.hclust <- hclust(d, method="ward.D2")
wisc.pr.hclust.clusters.4 <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters.4, diagnosis)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
table(wisc.pr.hclust.clusters.2, diagnosis)
table(wisc.pr.hclust.clusters.4, diagnosis)
```

> Q14. I think the hierchial cluster performed with ward.D2 method and 2 clusters is the best, as it identifies most of the malignant tumors correctly while keeping a low number of benign tumors misclassified.


## Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
plot(wisc.pr$x[,1:2], col = diagnosis ,
     xlab = "PC1", ylab = "PC2")
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q16. Patient 2 might need a prioritized check-up as it is clustered with malignant tumors, indicating the possibility of a malignant tumor for this patient. 

